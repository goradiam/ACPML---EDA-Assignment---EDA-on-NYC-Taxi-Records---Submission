{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNZCnDmNoUgh7hXaqB+jGp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goradiam/ACPML---EDA-Assignment---EDA-on-NYC-Taxi-Records---Submission/blob/main/class_practise_settion_for_model_training_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-VGisipqf3Q"
      },
      "outputs": [],
      "source": [
        "# prompt: we need to import sckitlearn for xg boosting , decision trees\n",
        "\n",
        "!pip install scikit-learn xgboost\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: analyze kyphosis.csv in the content folder as per the ensemble adaboost classifier estimator from sklearn ,is dt assume  no eda required use 20 percent split maxdepth 1 try using samme algorithm target column has two classes absent and present report the confusion display for train and test data\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('kyphosis.csv')\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('Kyphosis', axis=1)\n",
        "y = df['Kyphosis']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the base estimator (Decision Tree)\n",
        "dt_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost classifier\n",
        "adaboost = AdaBoostClassifier(estimator=dt_estimator,n_estimators=200, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = adaboost.predict(X_train)\n",
        "y_test_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Calculate and print confusion matrices\n",
        "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
        "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(\"Confusion Matrix (Training Data):\")\n",
        "print(conf_matrix_train)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Testing Data):\")\n",
        "conf_matrix_test"
      ],
      "metadata": {
        "id": "pQ_T3YByrILb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GheoAK3zxFT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient boosting"
      ],
      "metadata": {
        "id": "9EHm6VJixLeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: we are trying touse kyphosis.csv from content folder for gradient boosting based  ensemble classification report print the metric for confusion , use estimator as 200 train and test split as 80\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier\n",
        "# Using XGBoost as it's a popular and effective implementation of gradient boosting\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred_xgb = xgb_model.predict(X_train)\n",
        "y_test_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate and print confusion matrices for Gradient Boosting\n",
        "conf_matrix_train_xgb = confusion_matrix(y_train, y_train_pred_xgb)\n",
        "conf_matrix_test_xgb = confusion_matrix(y_test, y_test_pred_xgb)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Training Data - Gradient Boosting):\")\n",
        "print(conf_matrix_train_xgb)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Testing Data - Gradient Boosting):\")\n",
        "conf_matrix_test_xgb"
      ],
      "metadata": {
        "id": "AAC8BP_uxQGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "glCkFU6LyaE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# prompt: we need to import sckitlearn for xg boosting , decision trees\n",
        "\n",
        "!pip install scikit-learn xgboost\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "# %%\n",
        "# prompt: analyze kyphosis.csv in the content folder as per the ensemble adaboost classifier estimator from sklearn ,is dt assume  no eda required use 20 percent split maxdepth 1 try using samme algorithm target column has two classes absent and present report the confusion display for train and test data\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('kyphosis.csv')\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('Kyphosis', axis=1)\n",
        "y = df['Kyphosis']\n",
        "\n",
        "# Initialize and fit LabelEncoder to convert target variable to numerical labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y) # Create a new encoded target variable\n",
        "\n",
        "# Split the data into training and testing sets using the encoded target variable\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Initialize the base estimator (Decision Tree)\n",
        "dt_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost classifier\n",
        "# Use the original y_train and y_test for AdaBoost as it can handle string labels in some cases\n",
        "adaboost = AdaBoostClassifier(estimator=dt_estimator,n_estimators=200, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = adaboost.predict(X_train)\n",
        "y_test_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Calculate and print confusion matrices\n",
        "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
        "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(\"Confusion Matrix (Training Data):\")\n",
        "print(conf_matrix_train)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Testing Data):\")\n",
        "conf_matrix_test\n",
        "# %%\n",
        "\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier\n",
        "# Using XGBoost as it's a popular and effective implementation of gradient boosting\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the XGBoost model using the encoded target variable\n",
        "xgb_model.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Make predictions using the encoded test target variable\n",
        "y_train_pred_xgb = xgb_model.predict(X_train)\n",
        "y_test_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate and print confusion matrices for Gradient Boosting using the encoded target variable\n",
        "conf_matrix_train_xgb = confusion_matrix(y_train_encoded, y_train_pred_xgb)\n",
        "conf_matrix_test_xgb = confusion_matrix(y_test_encoded, y_test_pred_xgb)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Training Data - Gradient Boosting):\")\n",
        "print(conf_matrix_train_xgb)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Testing Data - Gradient Boosting):\")\n",
        "conf_matrix_test_xgb"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rNYPhABVycJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# %% [markdown]\n",
        "# #Gradient boosting\n",
        "# %%\n",
        "# prompt: we are trying touse kyphosis.csv from content folder for gradient boosting based  ensemble classification report print the metric for confusion , use estimator as 200 train and test split as 80\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier\n",
        "# Using XGBoost as it's a popular and effective implementation of gradient boosting\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the XGBoost model using the encoded target variable\n",
        "xgb_model.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Make predictions using the encoded test target variable\n",
        "y_train_pred_xgb = xgb_model.predict(X_train)\n",
        "y_test_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate and print confusion matrices for Gradient Boosting using the encoded target variable\n",
        "conf_matrix_train_xgb = confusion_matrix(y_train_encoded, y_train_pred_xgb)\n",
        "conf_matrix_test_xgb = confusion_matrix(y_test_encoded, y_test_pred_xgb)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Training Data - Gradient Boosting):\")\n",
        "print(conf_matrix_train_xgb)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Testing Data - Gradient Boosting):\")\n",
        "conf_matrix_test_xgb"
      ],
      "metadata": {
        "id": "uLqL5CHWy_41"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DHHPrswz4VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# prompt: how to  you use k fold technique for evaluating above model for different depths\n",
        "\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix # Although confusion matrix isn't directly output by xgb.cv, we can still calculate it after training on the full data with the best depth.\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('kyphosis.csv')\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('Kyphosis', axis=1)\n",
        "y = df['Kyphosis']\n",
        "\n",
        "# Encode the target variable to numerical labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Define the data in DMatrix format, which is optimized for XGBoost\n",
        "dtrain = xgb.DMatrix(X, label=y_encoded)\n",
        "\n",
        "# Define a range of max_depth values to evaluate\n",
        "depths_to_evaluate = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Dictionary to store the cross-validation results for each depth\n",
        "cv_results = {}\n",
        "\n",
        "# Perform k-fold cross-validation for each depth\n",
        "for depth in depths_to_evaluate:\n",
        "    print(f\"Evaluating with max_depth = {depth}\")\n",
        "    params = {\n",
        "        'objective': 'binary:logistic',  # Objective for binary classification\n",
        "        'eval_metric': 'logloss',       # Evaluation metric\n",
        "        'eta': 0.1,                     # Learning rate\n",
        "        'max_depth': depth,             # Maximum depth of the trees\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    # num_boost_round is the number of boosting rounds (estimators)\n",
        "    # nfold is the number of folds for cross-validation\n",
        "    # early_stopping_rounds can be used to stop training if performance on the validation set doesn't improve\n",
        "    cv_result = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=200, # Using the same number of estimators as before\n",
        "        nfold=5,             # Using 5 folds for cross-validation\n",
        "        metrics='logloss',\n",
        "        seed=42,\n",
        "        verbose_eval=False # Set to True to see evaluation progress\n",
        "    )\n",
        "\n",
        "    # Store the mean of the test metric (logloss) from the last boosting round\n",
        "    cv_results[depth] = cv_result['test-logloss-mean'].iloc[-1]\n",
        "\n",
        "# Print the cross-validation results for each depth\n",
        "print(\"\\nCross-validation results (mean logloss):\")\n",
        "for depth, mean_logloss in cv_results.items():\n",
        "    print(f\"Max Depth {depth}: {mean_logloss:.4f}\")\n",
        "\n",
        "# Determine the best depth based on the minimum logloss\n",
        "best_depth = min(cv_results, key=cv_results.get)\n",
        "print(f\"\\nBest max_depth based on cross-validation: {best_depth}\")\n",
        "\n",
        "# After finding the best depth, you would typically train the final model on the full dataset\n",
        "# using the best depth and the desired number of estimators.\n",
        "# Then you can evaluate its performance on the test set if you had one,\n",
        "# or use the cross-validation results as your performance estimate.\n",
        "\n",
        "# Example: Train a model with the best depth on the full dataset (for demonstration)\n",
        "# This is not strictly part of the cross-validation evaluation process itself,\n",
        "# but shows how you might use the best parameter found.\n",
        "# print(f\"\\nTraining final model with best depth ({best_depth}) on the full dataset...\")\n",
        "# final_xgb_model = xgb.XGBClassifier(n_estimators=200, max_depth=best_depth, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "# final_xgb_model.fit(X, y_encoded)\n",
        "\n",
        "# You could then evaluate this final model on a separate test set if available,\n",
        "# or use the cross-validation results as the primary performance indicator."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OppvjUksz475"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}